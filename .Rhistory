# Useful! Get counts of data from Excel =========================================================
# Get data (One column) from Clipboard ================================================
mycol = read.table(file="clipboard", sep=",") # sep="," for 1 column, sep="\t" for multiple columns
#write.table(mycol, "clipboard", sep="\t")
head(mycol)
nrow(mycol)
# Useful! Get counts of data from Excel =========================================================
# Get data (One column) from Clipboard ================================================
mycol = read.table(file="clipboard", sep=",") # sep="," for 1 column, sep="\t" for multiple columns
#write.table(mycol, "clipboard", sep="\t")
head(mycol)
nrow(mycol)
# Freq table with 2 variables.===========================================================
mydata = read.table(file="clipboard", sep="\t") # sep="," for 1 column, sep="\t" for multiple columns
# write.table(mycol, "clipboard", sep="\t")
# mydata
head(mydata)
tail(mydata)
nrow(mydata)
as.data.frame(table(mydata$V1))
V1table = as.data.frame(table(mydata$V1))
write.table(V1table, "clipboard", sep="\t")
write.table(V1table, "clipboard", sep="\t", row.names = F)
# Useful! Get counts of data from Excel =========================================================
# Get data (One column) from Clipboard ================================================
mycol = read.table(file="clipboard", sep=",") # sep="," for 1 column, sep="\t" for multiple columns
#write.table(mycol, "clipboard", sep="\t")
head(mycol)
nrow(mycol)
# mycol
bbb = as.data.frame(table(mycol))
bbb
# Useful! Get counts of data from Excel =========================================================
# Get data (One column) from Clipboard ================================================
mycol = read.table(file="clipboard", sep=",") # sep="," for 1 column, sep="\t" for multiple columns
#write.table(mycol, "clipboard", sep="\t")
head(mycol)
nrow(mycol)
# Useful! Get counts of data from Excel =========================================================
# Get data (One column) from Clipboard ================================================
mycol = read.table(file="clipboard", sep=",") # sep="," for 1 column, sep="\t" for multiple columns
#write.table(mycol, "clipboard", sep="\t")
head(mycol)
nrow(mycol)
# mycol
bbb = as.data.frame(table(mycol))
bbb
write.table(bbb, "clipboard", sep="\t", row.names = F)
# Useful! Get counts of data from Excel =========================================================
# Get data (One column) from Clipboard ================================================
mycol = read.table(file="clipboard", sep=",") # sep="," for 1 column, sep="\t" for multiple columns
#write.table(mycol, "clipboard", sep="\t")
head(mycol)
nrow(mycol)
# mycol
bbb = as.data.frame(table(mycol))
# mycol
bbb = as.data.frame(table(mycol))
bbb
write.table(bbb, "clipboard", sep="\t", row.names = F)
# Load necessary packages.
library(SASxport)
library(foreign)
setwd("~/GitHub/dietary_patterns")
# Set where the NHANES data and food code table are.
# it is not in the eg_data folder because it's too large to save in GitHub folder.
# setwd("E:/MSU OneDrive 20210829/UMinn/20_NHANES/2015-16")
setwd("~/GitHub/dietary_patterns")
# Load necessary functions.
source("/lib/load_clean_NHANES.R")
# Load necessary functions.
source("lib/load_clean_NHANES.R")
# Format the food table and save it as a .txt file.
PrepareFoodCodeTable(raw.food.code.table = "eg_data/NHANES/FoodCodes_DRXFCD_I.XPT",
out.fn =              "eg_data/NHANES/FoodCodes_DRXFCD_I_f.txt")
# Load the saved food items file.
Food_D1 <- read.table("eg_data/NHANES/Food_D1_w_code.txt", sep="\t", header=T)
# Load the formatted food code table.
foodcodetable_f <- read.table("eg_data/NHANES/FoodCodes_DRXFCD_I_f.txt", sep="\t", header=T)
# Import items data Day 1, add food item descriptions, and save it as a txt file.
# LIKELY IT WILL BE A HUGE FILE.
ImportNHANESFoodItems(data.name="E:/MSU OneDrive 20210829/UMinn/20_NHANES/2015-16/Data/Interview_IndFoods_Day1_DR1IFF_I.XPT",
food.code.column = "DR1IFDCD",
food.code.table = foodcodetable_f,
out.fn = "eg_data/NHANES/Food_D1_w_code.txt")
# Import items data Day 2, add food item descriptions, and save it as a txt file.
ImportNHANESFoodItems(data.name="E:/MSU OneDrive 20210829/UMinn/20_NHANES/2015-16/Data/Interview_IndFoods_Day2_DR2IFF_I.XPT",
food.code.column = "DR2IFDCD",
food.code.table = foodcodetable_f,
out.fn = "eg_data/NHANES/Food_D2_w_code.txt")
# Import items data Day 1, add food item descriptions, and save it as a txt file.
# LIKELY IT WILL BE A HUGE FILE.
ImportNHANESFoodItems(data.name="E:/MSU OneDrive 20210829/UMinn/20_NHANES/2015-16/Data/Interview_IndFoods_Day1_DR1IFF_I.XPT",
food.code.column = "DR1IFDCD",
food.code.table = foodcodetable_f,
out.fn = "eg_data/NHANES/Interview_IndFoods_Day1_DR1IFF_I_d.txt") # 'd' stands for food descriptions
# Import items data Day 2, add food item descriptions, and save it as a txt file.
ImportNHANESFoodItems(data.name="E:/MSU OneDrive 20210829/UMinn/20_NHANES/2015-16/Data/Interview_IndFoods_Day2_DR2IFF_I.XPT",
food.code.column = "DR2IFDCD",
food.code.table = foodcodetable_f,
out.fn = "eg_data/NHANES/Interview_IndFoods_Day2_DR2IFF_I_d.txt")
# Load total nutrient intake of day 1 or day 2. Day 1 has more columns that can be used as metadata.
nhanes1516_totals1 <- read.xport('E:/MSU OneDrive 20210829/UMinn/20_NHANES/2015-16/Data/Total_Nutrient_Day1_DR1TOT_J.XPT')
# ---------------------------------------------------------------------------------------------------------------
# For totals, the same QC can be applied as ASA24 totals QC procedure.
# Functions to clean ASA24 data.
source("lib/load_clean_ASA24.R")
# ---------------------------------------------------------------------------------------------------------------
# Save QCtotals as "Totals_QCed.txt"
write.table(QCtotals, "eg_data/NHANES/NHANES_totals_QCed.txt", sep="\t", quote=F, row.names=F)
setwd("~/GitHub/dietary_patterns")
# Load the necessary functions
source("lib/prep_data_for_clustering.R")
source("lib/PCA.R")
source("lib/k-means.R")
# Import source code to run the analyses to follow.
source("lib/specify_dir_and_check_col.R")
source("lib/prep_data_for_clustering.R")
source("lib/PCA.R")
# Name your main directory for future use.
main.wd <- file.path(getwd())
# Define ggplot themes to use in creating plots.
library(ggplot2)
ggplot2::theme_set(theme_bw(base_size = 14))
totals_QCed_sampled <- read.table("eg_data/NHANES/NHANES_totals_QCed_sampled_PCAs_18ind.txt", sep="\t", header=T)
totals_QCed_sampled <- read.table("E:/MSU OneDrive 20210829/UMinn/20_NHANES/2015-16NHANES_totals_QCed_sampled_PCAs_18ind.txt", sep="\t", header=T)
totals_QCed_sampled <- read.table("E:/MSU OneDrive 20210829/UMinn/20_NHANES/2015-16/NHANES_totals_QCed_sampled_PCAs_18ind.txt", sep="\t", header=T)
dim(totals_QCed_sampled)
head(totals_QCed_sampled,1)
grep("*THEO", totals_QCed_sampled,1)
# Load total nutrient intake of day 1 or day 2. Day 1 has more columns that can be used as metadata.
nhanes1516_totals1 <- read.xport('E:/MSU OneDrive 20210829/UMinn/20_NHANES/2015-16/Data/Total_Nutrient_Day1_DR1TOT_J.XPT')
colnames(nhanes1516_totals1)
# TOTALS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Define the input data to be used.
input_data <- totals_QCed_sampled
# The columns specified as start.col, end.col, and all columns in between will be selected.
# Totals  --> start.col = "DR1TPROT",    end.col = "DR1TP226"
SubsetColumns(data=input_data, start.col="DR1TPROT", end.col = "DR1TP226")
# TOTALS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Define the input data to be used.
input_data <- totals_QCed_sampled
# The columns specified as start.col, end.col, and all columns in between will be selected.
# Totals  --> start.col = "DR1TPROT",    end.col = "DR1TP226"
SubsetColumns(data=input_data, start.col="DR1TPROT", end.col = "DR1TP226")
# pick up only the columns with non-zero variance, in order to run PCA, cluster analysis etc.
# The removed columns will be shown if any.
KeepNonZeroVarColumns(data = subsetted)
# The out put is a df called "subsetted_non0var".
colnames(subsetted_non0var)
# ---------------------------------------------------------------------------------------------------------------
# Collapse variables by correlation: take only one variables if they are highly correlated.
cbc_res <- CollapseByCorrelation(x = subsetted_non0var,
min.cor = 0.75,
select.rep.fcn = 'mean', verbose = T)
# Filter out highly correlated variables from the original dataset.
selected_variables <- subsetted_non0var[, cbc_res$reps]
# Check to see the name of the original and filtered variables.
# Among the variables in the same group, the one with the highest variance is kept
#  (according to the explanation above.)
# filtered
head(selected_variables, 1)
dim(selected_variables)
# ---------------------------------------------------------------------------------------------------------------
# Save the correlation matrix for record in the results folder.
# cc is the correlation matrix produced when variables are collapsed by correlation.
SaveCorrMatrix(x=cc, out.fn = "results/PCA_results/NHANES_totals_QCed_sampled_PCAs_18ind_corr_matrix.txt")
# ========================================================================================
# Perform Principal Component Analysis.
# ========================================================================================
#
# ---------------------------------------------------------------------------------------------------------------
# Name your input data.
# Your input data should be a data frame with variables with non-zero variance.
pca_input <- selected_variables
dim(pca_input)
# Perform PCA with the subset data, scaled.
scaled_pca <- prcomp(x=pca_input, scale = T)
# Create a scree plot.
screep <- LineScreePlot(pca.data = pca_input, pca.result = scaled_pca)
screep
# Create a biplot.
# A biplot with the individuals as black dots and variables labelled.
biplotdots <- BiplotDots(pca.result = scaled_pca, pca.data = pca_input)
biplotdots
# A biplot with the individuals labeled.
biplotlabeled <- BiplotLabeled(pca.result=scaled_pca, pca.data=pca_input, individuals.label = T)
biplotlabeled
# A biplot with the individuals labeled without the variables' arrows.
biplotlabeledwoarrows <- BiplotLabeledwoArrows(pca.result = scaled_pca,
pca.data = pca_input,
individuals.label = T)
biplotlabeledwoarrows
# Plot the contribution of the variables to a given PC.
# Variables' labels aligned on the X axis.
loadings_aligned <- LoadingsPlot(pca.result=scaled_pca,  whichPC="PC1",
positive.color="green2", negative.color="grey70", labels.aligned= TRUE)
loadings_aligned
SaveInputAndPCs(input = "E:/MSU OneDrive 20210829/UMinn/20_NHANES/2015-16/NHANES_totals_QCed_sampled_PCAs_18ind.txt",
pca.results = scaled_pca,
out.fn = "results/PCA_results/18 ind/ind18_totalsinput_QCed_PCs_2.txt")
# Freq table with 2 variables.===========================================================
mydata = read.table(file="clipboard", sep="\t") # sep="," for 1 column, sep="\t" for multiple columns
# write.table(mycol, "clipboard", sep="\t")
# mydata
head(mydata)
tail(mydata)
# write.table(mycol, "clipboard", sep="\t")
# mydata
dim(mydata)
library(dplyr)
mydata = read.table(file="clipboard", sep="\t", header = T)
dim(mydata)
# write.table(mycol, "clipboard", sep="\t")
# mydata
head(mydata)
# Load the QC-ed and sampled totals file.
totals_QCed_sampled <- read.table("eg_data/NHANES/NHANES_totals_QCed_sampled.txt", sep="\t", header=T)
# Load the QC-ed and sampled totals file.
totals_QCed_sampled <- read.table("E:/MSU OneDrive 20210829/UMinn/20_NHANES/2015-16/NHANES_totals_QCed_sampled.txt", sep="\t", header=T)
dim(totals_QCed_sampled)                                                            NHANES_totals_QCed_sampled_PCAs_18ind_input.txt
totals_QCed_sampled
dim(totals_QCed_sampled)                                                            NHANES_totals_QCed_sampled_PCAs_18ind_input.txt
dim(totals_QCed_sampled)
head(totals_QCed_sampled,1)
# TOTALS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Define the input data to be used.
input_data <- totals_QCed_sampled
# The columns specified as start.col, end.col, and all columns in between will be selected.
# Totals  --> start.col = "DR1TPROT",    end.col = "DR1TP226"
SubsetColumns(data=input_data, start.col="DR1TPROT", end.col = "DR1TP226")
# pick up only the columns with non-zero variance, in order to run PCA, cluster analysis etc.
# The removed columns will be shown if any.
KeepNonZeroVarColumns(data = subsetted)
# The out put is a df called "subsetted_non0var".
colnames(subsetted_non0var)
# ---------------------------------------------------------------------------------------------------------------
# Collapse variables by correlation: take only one variables if they are highly correlated.
cbc_res <- CollapseByCorrelation(x = subsetted_non0var,
min.cor = 0.75,
select.rep.fcn = 'mean', verbose = T)
# Filter out highly correlated variables from the original dataset.
selected_variables <- subsetted_non0var[, cbc_res$reps]
# Check to see the name of the original and filtered variables.
# Among the variables in the same group, the one with the highest variance is kept
#  (according to the explanation above.)
# filtered
head(selected_variables, 1)
dim(selected_variables)
# ========================================================================================
# Perform Principal Component Analysis.
# ========================================================================================
#
# ---------------------------------------------------------------------------------------------------------------
# Name your input data.
# Your input data should be a data frame with variables with non-zero variance.
pca_input <- selected_variables
dim(pca_input)
# Perform PCA with the subset data, scaled.
scaled_pca <- prcomp(x=pca_input, scale = T)
# Create a scree plot.
screep <- LineScreePlot(pca.data = pca_input, pca.result = scaled_pca)
screep
# Create a biplot.
# A biplot with the individuals as black dots and variables labelled.
biplotdots <- BiplotDots(pca.result = scaled_pca, pca.data = pca_input)
biplotdots
# A biplot with the individuals labeled.
biplotlabeled <- BiplotLabeled(pca.result=scaled_pca, pca.data=pca_input, individuals.label = T)
biplotlabeled
ggsave("results/PCA_results/50 ind/50ind_screep.png", screep, device="png", width=5, height=5, dpi=200)
ggsave("results/PCA_results/50 ind/50ind_biplotdots.png", biplotdots, device="png", width=5, height=5, dpi=200)
# A biplot with the individuals labeled.
biplotlabeled <- BiplotLabeled(pca.result=scaled_pca, pca.data=pca_input, individuals.label = T)
biplotlabeled
ggsave("results/PCA_results/50 ind/50ind_biplotlabeled.png", biplotlabeled, device="png", width=5, height=5, dpi=200)
# A biplot with the individuals labeled without the variables' arrows.
biplotlabeledwoarrows <- BiplotLabeledwoArrows(pca.result = scaled_pca,
pca.data = pca_input,
individuals.label = T)
biplotlabeledwoarrows
ggsave("results/PCA_results/50 ind/50ind_biplotlabeledwoarrows.png", biplotlabeledwoarrows, device="png", width=5, height=5, dpi=200)
# Plot the directions of the variables.
directions <- BiplotLabeled(pca.result=scaled_pca, pca.data=pca_input, individuals.label=F)
directions
ggsave("results/PCA_results/50 ind/50ind_directions.png", directions, device="png", width=5, height=5, dpi=200)
# Plot the contribution of the variables to a given PC.
# Variables' labels aligned on the X axis.
loadings_aligned <- LoadingsPlot(pca.result=scaled_pca,  whichPC="PC1",
positive.color="green2", negative.color="grey70", labels.aligned= TRUE)
# Variables' labels are placed right below the bars.
LoadingsPlot(pca.result=scaled_pca,  whichPC="PC1",
positive.color="green2", negative.color="grey70", labels.aligned= FALSE)
# Variables' labels are placed right below the bars.
LoadingsPlot(pca.result=scaled_pca,  whichPC="PC2",
positive.color="green2", negative.color="grey70", labels.aligned= FALSE)
# Variables' labels are placed right below the bars.
LoadingsPlot(pca.result=scaled_pca,  whichPC="PC1",
positive.color="green2", negative.color="grey70", labels.aligned= FALSE)
# ---------------------------------------------------------------------------------------------------------------
# Save the variance explained by each PC as a .txt file.
# Change the file name as necessary.
SaveVarExplained(pca.data = pca_input, pca.result = scaled_pca,
out.fn = "results/PCA_results/50 ind/PC_var_explained_50ind.txt")
# ---------------------------------------------------------------------------------------------------------------
# Calculate loadings of each PC to the variables and
# save it as a txt file in the results folder.
# Change the file name as necessary.
SaveLoadings(pca.result=scaled_pca,
out.fn = "results/PCA_results/50 ind/PC_loadings_50ind.txt")
SaveInputAndPCs(input = "E:/MSU OneDrive 20210829/UMinn/20_NHANES/2015-16/NHANES_totals_QCed_sampled.txt",
pca.results = scaled_pca,
out.fn = "results/PCA_results/50 ind/ind50_totalsinput_QCed_PCs.txt")
# ---------------------------------------------------------------------------------------------------------------
# Save the correlation matrix for record in the results folder.
# cc is the correlation matrix produced when variables are collapsed by correlation.
SaveCorrMatrix(x=cc, out.fn = "results/PCA_results/50 ind/NHANES_totals_QCed_sampled_PCAs_50ind_corr_matrix.txt")
totals_QCed_sampled <- read.table("eg_data/NHANES/NHANES_totals_QCed_sampled_PCAs_18ind.txt", sep="\t", header=T)
dim(totals_QCed_sampled)
head(totals_QCed_sampled,1)
# FOOD ITEMS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Define the input data to be used.
input_data <- foods_QCed
# TOTALS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Define the input data to be used.
input_data <- totals_QCed_sampled
# The columns specified as start.col, end.col, and all columns in between will be selected.
# Totals  --> start.col = "DR1TPROT",    end.col = "DR1TP226"
SubsetColumns(data=input_data, start.col="DR1TPROT", end.col = "DR1TP226")
# pick up only the columns with non-zero variance, in order to run PCA, cluster analysis etc.
# The removed columns will be shown if any.
KeepNonZeroVarColumns(data = subsetted)
# The out put is a df called "subsetted_non0var".
colnames(subsetted_non0var)
# ---------------------------------------------------------------------------------------------------------------
# Collapse variables by correlation: take only one variables if they are highly correlated.
cbc_res <- CollapseByCorrelation(x = subsetted_non0var,
min.cor = 0.75,
select.rep.fcn = 'mean', verbose = T)
# Filter out highly correlated variables from the original dataset.
selected_variables <- subsetted_non0var[, cbc_res$reps]
# Save the variables after removing correlated variables
write.table("results/PCA_results/18 ind/variables_retained_18ind", subsetted_non0var, sep="\t", row.names=F, quote=F)
# Save the variables after removing correlated variables
write.table("results/PCA_results/18 ind/variables_retained_18ind.txt", subsetted_non0var, sep="\t", row.names=F, quote=F)
# Save the variables after removing correlated variables
write.table(subsetted_non0var, "results/PCA_results/18 ind/variables_retained_18ind.txt", sep="\t", row.names=F, quote=F)
# Check to see the name of the original and filtered variables.
# Among the variables in the same group, the one with the highest variance is kept
#  (according to the explanation above.)
# filtered
head(selected_variables, 1)
dim(selected_variables)
# Save the variables after removing correlated variables
write.table(selected_variables, "results/PCA_results/18 ind/variables_retained_18ind.txt", sep="\t", row.names=F, quote=F)
# ---------------------------------------------------------------------------------------------------------------
# Define your input file. Need to scale it to accommodate measurements in different units.
colnames(selected_variables)
kmeans_input <- scale(selected_variables) # correlated variables removed.
theme_set(theme_bw(base_size = 14))
# ---------------------------------------------------------------------------------------------------------------
# Use the elbow method to find the ideal K.
ElbowMethod(k.values = 1:15)
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K.  Uses cluster package.
SilhouetteMethod(k.values = 2:15)
# or use the factoextra package to use the Silhouette method.
factoextra::fviz_nbclust(kmeans_input, kmeans, method="silhouette")
# ---------------------------------------------------------------------------------------------------------------
# Use the Gap statistic method to find the ideal K.
set.seed(123)
GapMethod(k.values = 1:15)
# Or use the factoextra package to use the Gap statistic method.
set.seed(123)
FactoextraGapMethod(k.values = 1:15)
# ---------------------------------------------------------------------------------------------------------------
# Perform k-means analysis with one specified k.
One_K(myK = 2)
# ---------------------------------------------------------------------------------------------------------------
# Perform k-means analysis with one specified k.
One_K(myK = 5)
# ---------------------------------------------------------------------------------------------------------------
# Perform k-means analysis with one specified k.
One_K(myK = 4)
# ---------------------------------------------------------------------------------------------------------------
# Perform k-means analysis with one specified k.
One_K(myK = 6)
# ---------------------------------------------------------------------------------------------------------------
# Perform k-means analysis with multiple (2-4) Ks, and plot them in one window.
MultipleK(myKs = c(5,6,7,8))
# ---------------------------------------------------------------------------------------------------------------
# Use the elbow method to find the ideal K.
ElbowMethod(k.values = 1:15)
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K.  Uses cluster package.
SilhouetteMethod(k.values = 2:15)
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K.  Uses cluster package.
SilhouetteMethod(k.values = 2:15)
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K.  Uses cluster package.
SilhouetteMethod(k.values = 2:15)
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K.  Uses cluster package.
SilhouetteMethod(k.values = 2:15)
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K.  Uses cluster package.
SilhouetteMethod(k.values = 2:15)
# or use the factoextra package to use the Silhouette method.
factoextra::fviz_nbclust(kmeans_input, kmeans, method="silhouette")
# or use the factoextra package to use the Silhouette method.
factoextra::fviz_nbclust(kmeans_input, kmeans, method="silhouette")
# or use the factoextra package to use the Silhouette method.
factoextra::fviz_nbclust(kmeans_input, kmeans, method="silhouette")
# ---------------------------------------------------------------------------------------------------------------
# Use the elbow method to find the ideal K.
ElbowMethod(k.values = 1:10)
GapMethod(k.values = 1:15)
GapMethod(k.values = 1:15)
GapMethod(k.values = 1:15)
GapMethod(k.values = 1:15)
FactoextraGapMethod(k.values = 1:15)
FactoextraGapMethod(k.values = 1:15)
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K.  Uses cluster package.
SilhouetteMethod(k.values = 2:15)
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K.  Uses cluster package.
SilhouetteMethod(k.values = 2:15)
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K.  Uses cluster package.
SilhouetteMethod(k.values = 2:15)
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K.  Uses cluster package.
SilhouetteMethod(k.values = 2:15)
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K.  Uses cluster package.
SilhouetteMethod(k.values = 2:15)
# Or use the factoextra package to use the Gap statistic method.
set.seed(123)
FactoextraGapMethod(k.values = 1:15)
FactoextraGapMethod(k.values = 1:15)
FactoextraGapMethod(k.values = 1:15)
# ---------------------------------------------------------------------------------------------------------------
# Perform k-means analysis with multiple (2-4) Ks, and plot them in one window.
MultipleK(myKs = c(5,9,10,11))
