nrow(mycol)
# mycol
bbb = as.data.frame(table(mycol))
bbb
library(psych)
install.packages(psych)
install.packages("psych")
library(psych)
Holzinger.9
cor.plot(Holzinger.9,numbers=TRUE)
factanal(factors = 2, covmat = Holzinger.9, n.obs = 145,
rotation = "varimax")
ev <- eigen(Holzinger.9)
ev
fa <- factanal(factors = 2, covmat = Holzinger.9,n.obs = 145,
rotation = "varimax")
cluster.plot(fa)
cluster.plot(fa)
cluster.plot(fa)
# ------------------------------------------------------------------------------------------
# From the agricolae maintainer, Filipe de Mendiburu Delgado, 01/25/2022.
library(agricolae)
data(cotton)
cotton$lineage<-as.factor(cotton$lineage)
cotton$epoca<-as.factor(cotton$epoca)
# Build a mixed effect model with the block effect.
mymodel <- lmer(yield ~ lineage + (1|block), data = cotton)
# Build a mixed effect model with the block effect.
mymodel <- lme4::lmer(yield ~ lineage + (1|block), data = cotton)
out1<-anova(mymodel, type="3", test.statistic= "F")
out1<-agricolae::anova(mymodel, type="3", test.statistic= "F")
out1<-lme4::anova(mymodel, type="3", test.statistic= "F")
out1<-lmerTest::anova(mymodel, type="3", test.statistic= "F")
out1<-emmeans::anova(mymodel, type="3", test.statistic= "F")
# ------------------------------------------------------------------------------------------
# Load packages
library(agricolae)
data(cotton)
library(lm4)
library(lmerTest)
# Build a mixed effect model with the block effect.
mymodel <- lmer(yield ~ lineage + (1|block), data = cotton)
# Show anova table. But this does not show DFerror and MSerror that
# I need for LSD.test.
anova(mymodel,  type="3", test.statistic= "F")
# Build a mixed effect model with the block effect.
mymodel <- lme4::lmer(yield ~ lineage + (1|block), data = cotton)
out1<-emmeans::anova(mymodel, type="3", test.statistic= "F")
out1<-anova(mymodel, type="3", test.statistic= "F")
# Build a mixed effect model with the block effect.
mymodel <- lmer(yield ~ lineage + (1|block), data = cotton)
out1 <- anova(mymodel, type="3", test.statistic= "F")
out1
out2<-summary(mymodel)
out2
View(out2)
out3<-with(cotton, LSD.test(yield, lineage, DFerror, MSerror, console = TRUE))
DFerror <- out1$DenDF
MSerror <- out2$sigma^2
out3<-with(cotton, LSD.test(yield, lineage, DFerror, MSerror, console = TRUE))
install.packages("nlme")
install.packages("nlme")
install.packages("nlme")
install.packages("nlme")
install.packages("nlme")
data(pigs)
agricolae::data(cotton)
data(cotton)
library(agricolae)
data(cotton)
data(pigs)
library(emmeans)
data(pigs)
head(pigs)
head(pigs,10)
table(pigs$source)
table(pigs$percent)
# Use pigs data and build a model
mod3 = nlme::gls(conc ~ source, data = pigs,
weights = varIdent(form = ~1 | source)) # This part
library(gls)
library(nlme)
setwd("~/GitHub/dietary_patterns")
# Name your main directory for future use.
main.wd <- file.path(getwd())
# Import source code to run the analyses to follow.
source("lib/load_and_check.R")
source("lib/prep_data.R")
# ---------------------------------------------------------------------------------------------------------------
# Load example totals data
# Specify the directory where the data is.
SpecifyDataDirectory(directory.name = "eg_data/dietstudy/")
# Load the totals.csv
totals <- read.table("Totals_to_use.txt",  sep = "\t", header = T)
# Load your metadata if you have one.
metadata <- read.csv("Metadata_1.csv", header=T)
# Come back to the main directory
setwd(main.wd)
# Show which has "yes" in the "Remove" column, and remove them.
RemoveRows(data = totals, metadata.file = metadata)
# ---------------------------------------------------------------------------------------------------------------
# If taking average by each category (user or other treatment(s))
# Specify the data to be used, category to group by, and the range of columns (variables)
# to calculate the means of each variables
# Nutrients analysis  --> start.col = "PROT",    end.col = "B12_ADD"
AverageBy(data = totals_selected, by = "UserName", start.col = "PROT", end.col = "B12_ADD")
# Results are saved in this dataframe.  Probably too large to see as is.
meansbycategorydf
# The column names should be the same as start.col-end.col.
colnames(meansbycategorydf)
# The row names should be each category entry to calculate means for.
rownames(meansbycategorydf)
# ---------------------------------------------------------------------------------------------------------------
head(totals_selected$UserName, 10)
# ---------------------------------------------------------------------------------------------------------------
table(totals_selected$UserName)
# ---------------------------------------------------------------------------------------------------------------
# Function to QC rows of 'totals' by Metadata
# Show which has "yes" in the "Remove" column, and remove them.
RemoveRows <- function(data=totals, metadata.file=metadata){
toberemoved <<- subset(metadata.file, Remove=="yes")
cat(nrow(toberemoved), "rows below are to be removed:", "\n")
print(toberemoved)
# Merge the data and metadata.
merged <<- merge(x=data, y=metadata.file, by="UserName", all.x=T)
# Remove the rows that have "yes" in the "Remove" column.
totals_selected <<- subset(merged, Remove!="yes")
cat("The resulting file, totals_selected, has",
nrow(totals_selected), "rows and",
ncol(totals_selected), "columns.")
}
# Show which has "yes" in the "Remove" column, and remove them.
RemoveRows(data = totals, metadata.file = metadata)
# ---------------------------------------------------------------------------------------------------------------
table(totals_selected$UserName)
# Subset nutrients or food items data.
# The columns specified as start.col, end.col, and all columns in between will be selected.
# Nutrients analysis  --> start.col = "PROT",    end.col = "B12_ADD", 64 variablees in total.
SubsetColumns(data = totals_selected, start.col = "PROT",    end.col = "B12_ADD")
# pick up only the columns with non-zero variance, in order to run PCA, cluster analysis etc.
# The removed columns will be shown if any.
KeepNonZeroVarColumns(data = subsetted)
# "subsetted_non0var" is the dataframe to be used in the subsequent
# collapse by correlation procedure.
colnames(subsetted_non0var)
# ---------------------------------------------------------------------------------------------------------------
# Collapse variables by correlation: take only one variables if they are highly correlated.
cbc_res <- CollapseByCorrelation(x = subsetted_non0var,
min.cor = 0.75,
select.rep.fcn = 'mean', verbose = T)
# Filter out highly correlated variables from the original dataset.
selected_variables <-  subsetted_non0var[, cbc_res$reps]
# Check to see the name of the original and filtered variables.
# Among the variabels in the same group, the one with the highest variance is kept
#  (according to the explanation above.)
# filtered
head(selected_variables, 1)
dim(selected_variables)
# original
head(subsetted_non0var, 1)
dim(subsetted_non0var)
# ---------------------------------------------------------------------------------------------------------------
# Define your input file. Need to scale it to accommodate measurements in different units.
colnames(selected_variables)
kmeans_input <- scale(selected_variables) # correlated variables removed.
# Set your ggplot2 theme.
require(ggplot2)
theme_set(theme_bw(base_size = 14))
# ========================================================================================
# Find the ideal k
#  Modified code from https://uc-r.github.io/kmeans_clustering
# ========================================================================================
# ---------------------------------------------------------------------------------------------------------------
# Function to find the ideal k: Elbow method
set.seed(123)
# function to compute total within-cluster sum of square
wss <- function(k, data) {
kmeans(data, k, nstart = 25)$tot.withinss
}
# Compute and plot wss for k = 1 to k = 15
k.values <- 1:15
# extract wss for 2-15 clusters
wsstable <- data.frame(K=k.values, WithinClusterSS=NA)
for(i in k.values){
wssvalue <- wss(k.values[i], data = kmeans_input)
wsstable[i, 2] <- wssvalue
}
wsstable
ggplot(wsstable, aes(x = K, y = WithinClusterSS)) +
geom_line() +
geom_point() +
scale_x_continuous(breaks = 1:nrow(wsstable)) +
labs(x = "Number of clusters K",
y = "Total within-clusters sum of squares") +
theme(panel.grid.major = element_blank()) +
theme(panel.grid.minor = element_blank()) +
theme(axis.title.x = element_text(margin=margin(t = 10, r = 0, b = 0, l = 0) ) ) +
theme(axis.title.y = element_text(margin=margin(t = 0, r = 10, b = 0, l = 0) ) ) +
theme(aspect.ratio = 0.9)
# function to compute average silhouette for k clusters
avg_sil <- function(k) {
km.res <- kmeans(kmeans_input, centers=k, nstart=25)
ss <<- silhouette(km.res$cluster, dist(kmeans_input))
mean(ss[, 3])
}
# Compute and plot wss for k = 2 to k = 15
k.values <- 2:15
# extract avg silhouette for 2-15 clusters (using map_dbl function)
avg_sil_values <- purrr::map_dbl(k.values, avg_sil)
# ---------------------------------------------------------------------------------------------------------------
# Find the ideal k: the Silhouette method
# need cluster package
library(cluster)
# function to compute average silhouette for k clusters
avg_sil <- function(k) {
km.res <- kmeans(kmeans_input, centers=k, nstart=25)
ss <<- silhouette(km.res$cluster, dist(kmeans_input))
mean(ss[, 3])
}
# Compute and plot wss for k = 2 to k = 15
k.values <- 2:15
# extract avg silhouette for 2-15 clusters (using map_dbl function)
avg_sil_values <- purrr::map_dbl(k.values, avg_sil)
avg_sil_values
# Create a data frame with the sil values for plotting.
avg_sil_values_df <- data.frame(K=k.values, Avg_sil=avg_sil_values)
require(ggplot2)
ggplot(avg_sil_values_df, aes(x = K, y = Avg_sil)) +
geom_line() +
geom_point() +
scale_x_continuous(breaks = 1:nrow(avg_sil_values_df)) +
labs(x = "Number of clusters K",
y = "Average Silhouettes") +
theme(panel.grid.major = element_blank()) +
theme(panel.grid.minor = element_blank()) +
theme(axis.title.x = element_text(margin=margin(t = 10, r = 0, b = 0, l = 0) ) ) +
theme(axis.title.y = element_text(margin=margin(t = 0, r = 10, b = 0, l = 0) ) ) +
theme(aspect.ratio = 0.9)
# Or use factoextra package to use the silhouette method to identify the optimum K.
factoextra::fviz_nbclust(kmeans_input, kmeans, method="silhouette")
ggplot(avg_sil_values_df, aes(x = K, y = Avg_sil)) +
geom_line() +
geom_point() +
scale_x_continuous(breaks = 1:nrow(avg_sil_values_df)) +
labs(x = "Number of clusters K",
y = "Average Silhouettes") +
theme(panel.grid.major = element_blank()) +
theme(panel.grid.minor = element_blank()) +
theme(axis.title.x = element_text(margin=margin(t = 10, r = 0, b = 0, l = 0) ) ) +
theme(axis.title.y = element_text(margin=margin(t = 0, r = 10, b = 0, l = 0) ) ) +
theme(aspect.ratio = 0.9)
# Or use factoextra package to use the silhouette method to identify the optimum K.
factoextra::fviz_nbclust(kmeans_input, kmeans, method="silhouette")
gap_stat <- clusGap(kmeans_input, FUN = kmeans, nstart = 25,
K.max=15, #k.values[length(k.values)],
B=50) # B is the number of bootstrapping
# Print the result.
print(gap_stat, method = "firstmax")
# Convert the table to a dataframe first.
gap_stat_df <- as.data.frame(gap_stat[1])
# Add the number of clusters as a new column.
gap_stat_df$NumberofK <- k.values
k.values <- 1:15
# Convert the table to a dataframe first.
gap_stat_df <- as.data.frame(gap_stat[1])
# Add the number of clusters as a new column.
gap_stat_df$NumberofK <- k.values
# Plot the gap statistic with ggplot2
require(ggplot2)
ggplot(gap_stat_df, aes(x=NumberofK, y=Tab.gap)) +
geom_line() +
geom_point() +
geom_errorbar(aes(ymin=Tab.gap-Tab.SE.sim,
ymax=Tab.gap+Tab.SE.sim),
width=0.2,
position=position_dodge(0.05)) +
scale_x_continuous(breaks = 1:nrow(gap_stat_df)) +
labs(x = "Number of clusters K",
y = "Gap stastistic") +
theme(panel.grid.major = element_blank()) +
theme(panel.grid.minor = element_blank()) +
theme(axis.title.x = element_text(margin=margin(t = 10, r = 0, b = 0, l = 0) ) ) +
theme(axis.title.y = element_text(margin=margin(t = 0, r = 10, b = 0, l = 0) ) ) +
theme(aspect.ratio = 0.9)
# or use factoextra package.
factoextra::fviz_gap_stat(gap_stat)
myKs <- c(2, 3, 10, 15)
# Perform the k-means analysis, with the optimum number you found above as the 'centers'.
for(i in 1:length(myKs)){
km.results[[i]] <- kmeans(x=kmeans_input, centers = myKs[i], nstart = 25)
plots[[i]] = factoextra::fviz_cluster(km.results[[i]],
data = kmeans_input,
ellipse = T, ellipse.alpha = 0.1,
ggtheme = theme_bw(base_size = 10),
repel = F, labelsize = 10)
}
# ---------------------------------------------------------------------------------------------------------------
# Loop through multiple Ks
plots <- km.results <- list()
# ---------------------------------------------------------------------------------------------------------------
# Loop through multiple Ks
plots <- list()
km.results <- list()
myKs <- c(2, 3, 15)
myKs <- c(2, 3, 10, 15)
# Perform the k-means analysis, with the optimum number you found above as the 'centers'.
for(i in 1:length(myKs)){
km.results[[i]] <- kmeans(x=kmeans_input, centers = myKs[i], nstart = 25)
plots[[i]] = factoextra::fviz_cluster(km.results[[i]],
data = kmeans_input,
ellipse = T, ellipse.alpha = 0.1,
ggtheme = theme_bw(base_size = 10),
repel = F, labelsize = 10)
}
names(plots) <- c("K2", "K3", "K10", "K15")
grid.arrange(plots[[1]], plots[[2]], plots[[3]], plots[[4]], nrow = 2)
gridExtra::grid.arrange(plots[[1]], plots[[2]], plots[[3]], plots[[4]], nrow = 2)
# ---------------------------------------------------------------------------------------------------------------
# If taking average by each category (user or other treatment(s))
# Specify the data to be used, category to group by, and the range of columns (variables)
# to calculate the means of each variables
# Nutrients analysis  --> start.col = "PROT",    end.col = "B12_ADD"
AverageBy(data = totals_selected, by = "UserName", start.col = "PROT", end.col = "B12_ADD")
# Food items analysis --> start.col = "F_TOTAL", end.col = "A_DRINKS"
AverageBy(data = totals_selected, by = "UserName", start.col = "F_TOTAL", end.col = "A_DRINKS")
# Results are saved in this dataframe.  Probably too large to see as is.
meansbycategorydf
# The column names should be the same as start.col-end.col.
colnames(meansbycategorydf)
# The row names should be each category entry to calculate means for.
rownames(meansbycategorydf)
# pick up only the columns with non-zero variance, in order to run PCA, cluster analysis etc.
# The removed columns will be shown if any.
KeepNonZeroVarColumns(data = meansbycategorydf)
# ---------------------------------------------------------------------------------------------------------------
# Collapse variables by correlation: take only one variables if they are highly correlated.
cbc_res <- CollapseByCorrelation(x = subsetted_non0var,
min.cor = 0.75,
select.rep.fcn = 'mean', verbose = T)
# Filter out highly correlated variables from the original dataset.
selected_variables <-  subsetted_non0var[, cbc_res$reps]
# Check to see the name of the original and filtered variables.
# Among the variabels in the same group, the one with the highest variance is kept
#  (according to the explanation above.)
# filtered
head(selected_variables, 1)
dim(selected_variables)
# original
head(subsetted_non0var, 1)
dim(subsetted_non0var)
# ---------------------------------------------------------------------------------------------------------------
# Define your input file. Need to scale it to accommodate measurements in different units.
colnames(selected_variables)
kmeans_input <- scale(selected_variables) # correlated variables removed.
theme_set(theme_bw(base_size = 14))
# ========================================================================================
# Find the ideal k
#  Modified code from https://uc-r.github.io/kmeans_clustering
# ========================================================================================
# ---------------------------------------------------------------------------------------------------------------
# Function to find the ideal k: Elbow method
set.seed(123)
# function to compute total within-cluster sum of square
wss <- function(k, data) {
kmeans(data, k, nstart = 25)$tot.withinss
}
# Compute and plot wss for k = 1 to k = 15
k.values <- 1:15
# extract wss for 2-15 clusters
wsstable <- data.frame(K=k.values, WithinClusterSS=NA)
for(i in k.values){
wssvalue <- wss(k.values[i], data = kmeans_input)
wsstable[i, 2] <- wssvalue
}
wsstable
# Plot the within-clusters SS for each K, and look for the elbow
# (= the first k at which the SS of k+1 is minimal)
require(ggplot2)
ggplot(wsstable, aes(x = K, y = WithinClusterSS)) +
geom_line() +
geom_point() +
scale_x_continuous(breaks = 1:nrow(wsstable)) +
labs(x = "Number of clusters K",
y = "Total within-clusters sum of squares") +
theme(panel.grid.major = element_blank()) +
theme(panel.grid.minor = element_blank()) +
theme(axis.title.x = element_text(margin=margin(t = 10, r = 0, b = 0, l = 0) ) ) +
theme(axis.title.y = element_text(margin=margin(t = 0, r = 10, b = 0, l = 0) ) ) +
theme(aspect.ratio = 0.9)
# ---------------------------------------------------------------------------------------------------------------
# Find the ideal k: the Silhouette method
# need cluster package
library(cluster)
# function to compute average silhouette for k clusters
avg_sil <- function(k) {
km.res <- kmeans(kmeans_input, centers=k, nstart=25)
ss <<- silhouette(km.res$cluster, dist(kmeans_input))
mean(ss[, 3])
}
# Compute and plot wss for k = 2 to k = 15
k.values <- 2:15
# extract avg silhouette for 2-15 clusters (using map_dbl function)
avg_sil_values <- purrr::map_dbl(k.values, avg_sil)
avg_sil_values
# Create a data frame with the sil values for plotting.
avg_sil_values_df <- data.frame(K=k.values, Avg_sil=avg_sil_values)
ggplot(avg_sil_values_df, aes(x = K, y = Avg_sil)) +
geom_line() +
geom_point() +
scale_x_continuous(breaks = 1:nrow(avg_sil_values_df)) +
labs(x = "Number of clusters K",
y = "Average Silhouettes") +
theme(panel.grid.major = element_blank()) +
theme(panel.grid.minor = element_blank()) +
theme(axis.title.x = element_text(margin=margin(t = 10, r = 0, b = 0, l = 0) ) ) +
theme(axis.title.y = element_text(margin=margin(t = 0, r = 10, b = 0, l = 0) ) ) +
theme(aspect.ratio = 0.9)
# Or use factoextra package to use the silhouette method to identify the optimum K.
factoextra::fviz_nbclust(kmeans_input, kmeans, method="silhouette")
# Compute and plot wss for k = 2 to k = 15
k.values <- 2:15
# extract avg silhouette for 2-15 clusters (using map_dbl function)
avg_sil_values <- purrr::map_dbl(k.values, avg_sil)
avg_sil_values
# Create a data frame with the sil values for plotting.
avg_sil_values_df <- data.frame(K=k.values, Avg_sil=avg_sil_values)
ggplot(avg_sil_values_df, aes(x = K, y = Avg_sil)) +
geom_line() +
geom_point() +
scale_x_continuous(breaks = 1:nrow(avg_sil_values_df)) +
labs(x = "Number of clusters K",
y = "Average Silhouettes") +
theme(panel.grid.major = element_blank()) +
theme(panel.grid.minor = element_blank()) +
theme(axis.title.x = element_text(margin=margin(t = 10, r = 0, b = 0, l = 0) ) ) +
theme(axis.title.y = element_text(margin=margin(t = 0, r = 10, b = 0, l = 0) ) ) +
theme(aspect.ratio = 0.9)
# Or use factoextra package to use the silhouette method to identify the optimum K.
factoextra::fviz_nbclust(kmeans_input, kmeans, method="silhouette")
ggplot(avg_sil_values_df, aes(x = K, y = Avg_sil)) +
geom_line() +
geom_point() +
scale_x_continuous(breaks = 1:nrow(avg_sil_values_df)) +
labs(x = "Number of clusters K",
y = "Average Silhouettes") +
theme(panel.grid.major = element_blank()) +
theme(panel.grid.minor = element_blank()) +
theme(axis.title.x = element_text(margin=margin(t = 10, r = 0, b = 0, l = 0) ) ) +
theme(axis.title.y = element_text(margin=margin(t = 0, r = 10, b = 0, l = 0) ) ) +
theme(aspect.ratio = 0.9)
# Or use factoextra package to use the silhouette method to identify the optimum K.
factoextra::fviz_nbclust(kmeans_input, kmeans, method="silhouette")
k.values <- 1:15
# Calculate the gap statistic.
library(cluster)
gap_stat <- clusGap(kmeans_input, FUN = kmeans, nstart = 25,
K.max=15, #k.values[length(k.values)],
B=50) # B is the number of bootstrapping
# Print the result.
print(gap_stat, method = "firstmax")
# Convert the table to a dataframe first.
gap_stat_df <- as.data.frame(gap_stat[1])
# Add the number of clusters as a new column.
gap_stat_df$NumberofK <- k.values
# Plot the gap statistic with ggplot2
require(ggplot2)
ggplot(gap_stat_df, aes(x=NumberofK, y=Tab.gap)) +
geom_line() +
geom_point() +
geom_errorbar(aes(ymin=Tab.gap-Tab.SE.sim,
ymax=Tab.gap+Tab.SE.sim),
width=0.2,
position=position_dodge(0.05)) +
scale_x_continuous(breaks = 1:nrow(gap_stat_df)) +
labs(x = "Number of clusters K",
y = "Gap stastistic") +
theme(panel.grid.major = element_blank()) +
theme(panel.grid.minor = element_blank()) +
theme(axis.title.x = element_text(margin=margin(t = 10, r = 0, b = 0, l = 0) ) ) +
theme(axis.title.y = element_text(margin=margin(t = 0, r = 10, b = 0, l = 0) ) ) +
theme(aspect.ratio = 0.9)
# or use factoextra package.
factoextra::fviz_gap_stat(gap_stat)
# ---------------------------------------------------------------------------------------------------------------
# Loop through multiple Ks
plots <- list()
km.results <- list()
myKs <- c(2, 3, 15)
myKs <- c(2, 3, 10, 15)
# Perform the k-means analysis, with the optimum number you found above as the 'centers'.
for(i in 1:length(myKs)){
km.results[[i]] <- kmeans(x=kmeans_input, centers = myKs[i], nstart = 25)
plots[[i]] = factoextra::fviz_cluster(km.results[[i]],
data = kmeans_input,
ellipse = T, ellipse.alpha = 0.1,
ggtheme = theme_bw(base_size = 10),
repel = F, labelsize = 10)
}
names(plots) <- c("K2", "K3", "K10", "K15")
gridExtra::grid.arrange(plots[[1]], plots[[2]], plots[[3]], plots[[4]], nrow = 2)
myKs <- c(2, 3, 6, 15)
# Perform the k-means analysis, with the optimum number you found above as the 'centers'.
for(i in 1:length(myKs)){
km.results[[i]] <- kmeans(x=kmeans_input, centers = myKs[i], nstart = 25)
plots[[i]] = factoextra::fviz_cluster(km.results[[i]],
data = kmeans_input,
ellipse = T, ellipse.alpha = 0.1,
ggtheme = theme_bw(base_size = 10),
repel = F, labelsize = 10)
}
names(plots) <- c("K2", "K3", "K6", "K15")
gridExtra::grid.arrange(plots[[1]], plots[[2]], plots[[3]], plots[[4]], nrow = 2)
