# Useful! Get counts of data from Excel =========================================================
# Get data (One column) from Clipboard ================================================
mycol = read.table(file="clipboard", sep=",") # sep="," for 1 column, sep="\t" for multiple columns
#write.table(mycol, "clipboard", sep="\t")
head(mycol)
nrow(mycol)
# Useful! Get counts of data from Excel =========================================================
# Get data (One column) from Clipboard ================================================
mycol = read.table(file="clipboard", sep=",") # sep="," for 1 column, sep="\t" for multiple columns
#write.table(mycol, "clipboard", sep="\t")
head(mycol)
nrow(mycol)
# Freq table with 2 variables.===========================================================
mydata = read.table(file="clipboard", sep="\t") # sep="," for 1 column, sep="\t" for multiple columns
# write.table(mycol, "clipboard", sep="\t")
# mydata
head(mydata)
tail(mydata)
nrow(mydata)
as.data.frame(table(mydata$V1))
V1table = as.data.frame(table(mydata$V1))
write.table(V1table, "clipboard", sep="\t")
write.table(V1table, "clipboard", sep="\t", row.names = F)
# Useful! Get counts of data from Excel =========================================================
# Get data (One column) from Clipboard ================================================
mycol = read.table(file="clipboard", sep=",") # sep="," for 1 column, sep="\t" for multiple columns
#write.table(mycol, "clipboard", sep="\t")
head(mycol)
nrow(mycol)
# mycol
bbb = as.data.frame(table(mycol))
bbb
# Useful! Get counts of data from Excel =========================================================
# Get data (One column) from Clipboard ================================================
mycol = read.table(file="clipboard", sep=",") # sep="," for 1 column, sep="\t" for multiple columns
#write.table(mycol, "clipboard", sep="\t")
head(mycol)
nrow(mycol)
# Useful! Get counts of data from Excel =========================================================
# Get data (One column) from Clipboard ================================================
mycol = read.table(file="clipboard", sep=",") # sep="," for 1 column, sep="\t" for multiple columns
#write.table(mycol, "clipboard", sep="\t")
head(mycol)
nrow(mycol)
# mycol
bbb = as.data.frame(table(mycol))
bbb
write.table(bbb, "clipboard", sep="\t", row.names = F)
# Useful! Get counts of data from Excel =========================================================
# Get data (One column) from Clipboard ================================================
mycol = read.table(file="clipboard", sep=",") # sep="," for 1 column, sep="\t" for multiple columns
#write.table(mycol, "clipboard", sep="\t")
head(mycol)
nrow(mycol)
# mycol
bbb = as.data.frame(table(mycol))
# mycol
bbb = as.data.frame(table(mycol))
bbb
write.table(bbb, "clipboard", sep="\t", row.names = F)
# Pick up the SEQN that are present in both days.
merged <- merge(x= nhanes_totals_1.subset, y= nhanes_totals_2.subset)
# Set where the NHANES data and food code table are.
# it is not in the eg_data folder because it's too large to save in GitHub folder.
# setwd("E:/MSU OneDrive 20210829/UMinn/20_NHANES/2015-16")
setwd("~/GitHub/dietary_patterns")
# Load necessary packages.
library(SASxport)
library(foreign)
# Load necessary functions.
source("lib/load_clean_NHANES.R")
# Format the food table and save it as a .txt file.
PrepareFoodCodeTable(raw.food.code.table = "eg_data/NHANES/FoodCodes_DRXFCD_I.XPT",
out.fn =              "eg_data/NHANES/FoodCodes_DRXFCD_I_f.txt")
# Load total nutrient intake of day 1 or day 2. Day 1 has more columns that can be used as metadata.
nhanes1516_totals1 <- read.xport('E:/MSU OneDrive 20210829/UMinn/20_NHANES/2015-16/Data/Total_Nutrient_Day1_DR1TOT_J.XPT')
nhanes1516_totals2 <- read.xport('E:/MSU OneDrive 20210829/UMinn/20_NHANES/2015-16/Data/Total_Nutrient_Day2_DR2TOT_J.XPT')
# How many participants are in the total dataset?
length(unique(nhanes1516_totals1$SEQN))   # 8704 for totals day 1.
length(unique(nhanes1516_totals2$SEQN))   # 8704 for totals day 2.
# Take only DR1DRSTZ = 1
nhanes_totals_1 <- subset(nhanes1516_totals1, DR1DRSTZ == 1)
nhanes_totals_2 <- subset(nhanes1516_totals2, DR2DRSTZ == 1)
# How many participants selected?
length(unique(nhanes_totals_1$SEQN))
length(unique(nhanes_totals_2$SEQN))
# import variable names in totals file
# Day 1
day1variables <- read.table('E:/MSU OneDrive 20210829/UMinn/20_NHANES/2015-16/Data/NHANES_VarNames_Day1.txt', header=F)
head(day1variables)
# Which variables to pick up from the totals data
names.to.use <- names(nhanes_totals_1) %in% day1variables$V1
# pick up only the specified variables
nhanes_totals_1.subset <- nhanes_totals_1[, names.to.use]
# Add a column that says "Day 1"
nhanes_totals_1.subset$Day <- "Day1"
# Remove "DR1T", "DR1" from the column names
colnames(nhanes_totals_1.subset) <- gsub(colnames(nhanes_totals_1.subset), pattern = "DR1T", replacement = "")
colnames(nhanes_totals_1.subset) <- gsub(colnames(nhanes_totals_1.subset), pattern = "DR1", replacement = "")
# Check
head(nhanes_totals_1.subset, 1)
# Do the same for Day 2.
day2variables <- read.table('E:/MSU OneDrive 20210829/UMinn/20_NHANES/2015-16/Data/NHANES_VarNames_Day2.txt', header=F)
head(day2variables)
names.to.use <- names(nhanes_totals_2) %in% day2variables$V1
nhanes_totals_2.subset <- nhanes_totals_2[, names.to.use]
nhanes_totals_2.subset$Day <- "Day2"
colnames(nhanes_totals_2.subset) <- gsub(colnames(nhanes_totals_2.subset), pattern = "DR2T", replacement = "")
colnames(nhanes_totals_2.subset) <- gsub(colnames(nhanes_totals_2.subset), pattern = "DR2", replacement = "")
head(nhanes_totals_2.subset, 1)
# Make a data frame that has the column names of day 1 and day 2 totals, ensure they match.
colnames <- data.frame(day1=colnames(nhanes_totals_1.subset), day2=colnames(nhanes_totals_2.subset))
head(colnames)
identical(x=colnames$day1, y=colnames$day2)
# Create a long table
bound <- rbind(nhanes_totals_1.subset, nhanes_totals_2.subset)
bound
# Pick up the SEQN that are present in both days.
merged <- merge(x= nhanes_totals_1.subset, y= nhanes_totals_2.subset)
head(bound)
tail(bound)
table(bound$SEQN)
SEQNtable <- as.data.frame(table(bound$SEQN))
SEQNtable[order(SEQNtable$Freq, decreasing = T), ]
SEQNtable[order(SEQNtable$Freq, decreasing = F), ]
orderedSEQNtable <- SEQNtable[order(SEQNtable$Freq, decreasing = F), ]
head(orderedSEQNtable)
table(orderedSEQNtable$Freq)
# How many participants selected?
length(unique(nhanes_totals_1$SEQN))
length(unique(nhanes_totals_2$SEQN))
matched = merge(x=nhanes_totals_2, y=nhanes_totals_1, by = "SEQN", all.x = T)
length(unique(matched$SEQN)) # So, day 1 data is all included in Day 2 data.
head(nhanes_totals_1, 1)
length(unique(matched$SEQN)) # So, day 1 data is all included in Day 2 data.
head(orderedSEQNtable)
table(orderedSEQNtable$Freq)
matchedall2 = merge(x=nhanes_totals_2, y=nhanes_totals_1, by = "SEQN", all.x = T)
length(unique(matchedall2$SEQN)) # So, day 1 data is all included in Day 2 data.
matchedall = merge(x=nhanes_totals_2, y=nhanes_totals_1, by = "SEQN")
length(unique(matchedall$SEQN)) # So, day 1 data is all included in Day 2 data.
length(unique(matchedall$SEQN)) # So, there are 6491 participants who have both day 1 and day 2 data.
length(unique(matchedall$SEQN)) # So, there are 6491 participants who have both day 1 and day 2 data.
# Pick up the SEQN that are present in both days.
merged <- merge(x= nhanes_totals_1.subset, y= nhanes_totals_2.subset)
dim(merged)
# Create a long table
bound <- rbind(nhanes_totals_1.subset, nhanes_totals_2.subset)
dim(bound)
SEQNtable <- as.data.frame(table(bound$SEQN))
orderedSEQNtable <- SEQNtable[order(SEQNtable$Freq, decreasing = F), ]
table(orderedSEQNtable$Freq)
table(orderedSEQNtable$Freq)
table(SEQNtable$Freq)
# Sort the freq table.
orderedSEQNtable <- SEQNtable[order(SEQNtable$Freq, decreasing = F), ]
head(orderedSEQNtable)
# Sort the freq table.
orderedSEQNtable <- SEQNtable[order(SEQNtable$Freq, decreasing = T), ]
head(orderedSEQNtable)
# Pick up the SEQN that are present in both days.
SEQNs_to_use <- subset(orderedSEQNtable, Freq==2)
SEQNs_to_use
# Pick up the SEQN that are present in both days.
orderedSEQNtable_2 <- subset(orderedSEQNtable, Freq==2)
# This vector has only the SEQN to be used (picked up).
SEQNs_to_use <- orderedSEQNtable_2$Var1
# Pick up the SEQN that are present in both days.
orderedSEQNtable_2 <- subset(orderedSEQNtable, Freq==2)
dim(orderedSEQNtable_2)
# This vector has only the SEQN to be used (picked up).
SEQNs_to_use <- orderedSEQNtable_2$Var1
dim(SEQNs_to_use)
length(SEQNs_to_use)
# This vector has only the 6491 SEQNs to be used (picked up).
SEQNs_to_use <- orderedSEQNtable_2$Var1
SEQNs_to_use
orderedSEQNtable_2
# This vector has only the 6491 SEQNs to be used (picked up).
SEQNs_to_use <- orderedSEQNtable_2$Var1
SEQNs_to_use
orderedSEQNtable_2
head(bound)
# Select only the SEQN that are in orderedSEQNtable_2.
merged2 <- merge(x=orderedSEQNtable_2, y=bound, all.x = T)
orderedSEQNtable_2
dim(orderedSEQNtable_2)
head(orderedSEQNtable_2)
colnames(orderedSEQNtable_2)
colnames(orderedSEQNtable_2)[1] <- "SEQN"
head(bound)
# Select only the SEQN that are in orderedSEQNtable_2.
merged2 <- merge(x=orderedSEQNtable_2, y=bound, by="SEQN", all.x = T)
dim(merged2)
table(SEQNtable$Freq)
head(merged2)
table(merged2$Day)
# Select only the SEQN that are in orderedSEQNtable_2.
twodays <- merge(x=orderedSEQNtable_2, y=bound, by="SEQN", all.x = T)
head(twodays)
table(twodays$Day)
# Then take average of Day 1 and Day 2.
meantotals <- aggregate(twodays[, 3:67], by=list(twodays$SEQN), FUN=mean)
dim(meantotals)
head(meantotals)
head(twodays)
colnames(meantotals)[1] <- "SEQN"
head(meantotals)
subset(meantotals, SEQN=="100001")
head(twodays)
subset(meantotals, SEQN=="100001")
head(twodays,2)
table(twodays$Day)
subset(meantotals, SEQN=="100002")
head(twodays,4)
# Compare food amount consumed to the amount for yesterday
table(nhanes_totals_1$DR1_300)
# Save meantotals as a txt file.
write.table(meantotals, "data/NHANES1516_totals_2daymean.txt", sep="\t", row.names=F, quote=F)
# Load the meantotals from next session --
nhanes2days <- read.table("data/NHANES1516_totals_2daymean.txt", sep="\t", header=T)
# ---------------------------------------------------------------------------------------------------------------
# For totals, the same QC can be applied as ASA24 totals QC procedure.
# Functions to clean ASA24 data.
source("lib/load_clean_ASA24.R")
# Define the input data.  This will be modified after each filter.
QCtotals <- nhanes2days
# Flag if KCAL is <600 or >5700 --> ask remove or not --> if yes, remove those rows
QCOutliers(input.data = QCtotals,
target.colname = "DR1TKCAL", min = 600, max = 5700)
# Flag if KCAL is <600 or >5700 --> ask remove or not --> if yes, remove those rows
QCOutliers(input.data = QCtotals,
target.colname = "KCAL", min = 600, max = 5700)
# Flag if PROT is <10 or >240 --> ask remove or not --> if yes, remove those rows
QCOutliers(input.data = QCtotals,
target.colname = "PROT", min = 10, max = 240)
# Flag if TFAT is <15 or >230 --> ask remove or not --> if yes, remove those rows
QCOutliers(input.data = QCtotals,
target.colname = "TFAT", min = 15, max = 230)
# Flag if VC (Vitamin C) is <5 or >400 --> ask remove or not --> if yes, remove those rows
QCOutliers(input.data = QCtotals,
target.colname = "VC", min = 5, max = 400)
# Flag if BCAR (beta-carotene) is <15 or >8200 --> ask remove or not --> if yes, remove those rows
QCOutliers(input.data = QCtotals,
target.colname = "BCAR", min = 15, max = 8200)
# or show the outliers if too many.
bcaroutliers <- Outlier_rows[, c('SEQN', 'KCAL', 'BCAR')]
# Show the first n rows of the outliers in a descending order.
head(bcaroutliers[order(bcaroutliers$DR1TBCAR, decreasing = T), ], n=10)
# Show the first n rows of the outliers in a descending order.
head(bcaroutliers[order(bcaroutliers$BCAR, decreasing = T), ], n=10)
# Do the same for Day 2.
day2variables <- read.table('eg_data/NHANES/NHANES_VarNames_Day2.txt', header=F)
# ---------------------------------------------------------------------------------------------------------------
# Save QCtotals as "Totals_QCed.txt"
write.table(QCtotals, "eg_data/NHANES/NHANES_totals_2days_QCed.txt", sep="\t", quote=F, row.names=F)
# ---------------------------------------------------------------------------------------------------------------
# Take n random samples of participants.
RandomSample(data = QCtotals, n=50, out.fn = "eg_data/NHANES/NHANES_2days_totals_QCed_sampled.txt")
# Load the subsetted totals file.
totals_QCed_sampled <- read.table("eg_data/NHANES/NHANES_2days_totals_QCed_sampled.txt", sep="\t", header=T)
# Load the necessary functions
source("lib/prep_data_for_clustering.R")
source("lib/PCA.R")
source("lib/k-means.R")
dim(totals_QCed_sampled)
head(totals_QCed_sampled,1)
# FOOD ITEMS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Define the input data to be used.
input_data <- foods_QCed
# TOTALS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Define the input data to be used.
input_data <- totals_QCed_sampled
# The columns specified as start.col, end.col, and all columns in between will be selected.
# Totals  --> start.col = "KCAL",    end.col = "P226"
SubsetColumns(data=input_data, start.col="KCAL", end.col = "P226")
# pick up only the columns with non-zero variance, in order to run PCA, cluster analysis etc.
# The removed columns will be shown if any.
KeepNonZeroVarColumns(data = subsetted)
# The out put is a df called "subsetted_non0var".
colnames(subsetted_non0var)
# ---------------------------------------------------------------------------------------------------------------
# Collapse variables by correlation: take only one variables if they are highly correlated.
cbc_res <- CollapseByCorrelation(x = subsetted_non0var,
min.cor = 0.75,
select.rep.fcn = 'mean', verbose = T)
# Filter out highly correlated variables from the original dataset.
selected_variables <- subsetted_non0var[, cbc_res$reps]
# Check to see the name of the original and filtered variables.
# Among the variables in the same group, the one with the highest variance is kept
#  (according to the explanation above.)
# filtered
head(selected_variables, 1)
dim(selected_variables)
# original
head(subsetted_non0var, 1)
# Save the variables after removing correlated variables
write.table(selected_variables, "results/PCA_results/2 days 50 ind/variables_retained_2day_50.txt", sep="\t", row.names=F, quote=F)
# ---------------------------------------------------------------------------------------------------------------
# Save the correlation matrix for record in the results folder.
# cc is the correlation matrix produced when variables are collapsed by correlation.
SaveCorrMatrix(x=cc, out.fn = "results/PCA_results/2 days 50 ind/NHANES_2days_totals_QCed_sampled_50ind_corr_matrix.txt")
# Import source code to run the analyses to follow.
source("lib/specify_dir_and_check_col.R")
source("lib/prep_data_for_clustering.R")
source("lib/PCA.R")
# Define ggplot themes to use in creating plots.
library(ggplot2)
ggplot2::theme_set(theme_bw(base_size = 14))
# ========================================================================================
# Perform Principal Component Analysis.
# ========================================================================================
#
# ---------------------------------------------------------------------------------------------------------------
# Name your input data.
# Your input data should be a data frame with variables with non-zero variance.
pca_input <- selected_variables
dim(pca_input)
# Perform PCA with the subset data, scaled.
scaled_pca <- prcomp(x=pca_input, scale = T)
# Create a scree plot.
screep <- LineScreePlot(pca.data = pca_input, pca.result = scaled_pca)
screep
ggsave("results/PCA_results/50 ind/50ind_screep.png", screep, device="png", width=5, height=5, dpi=200)
# Create a biplot.
# A biplot with the individuals as black dots and variables labelled.
biplotdots <- BiplotDots(pca.result = scaled_pca, pca.data = pca_input)
biplotdots
# A biplot with the individuals labeled.
biplotlabeled <- BiplotLabeled(pca.result=scaled_pca, pca.data=pca_input, individuals.label = T)
biplotlabeled
# A biplot with the individuals labeled without the variables' arrows.
biplotlabeledwoarrows <- BiplotLabeledwoArrows(pca.result = scaled_pca,
pca.data = pca_input,
individuals.label = T)
biplotlabeledwoarrows
# Plot the directions of the variables.
directions <- BiplotLabeled(pca.result=scaled_pca, pca.data=pca_input, individuals.label=F)
directions
# Plot the directions of the variables.
directions <- BiplotLabeled(pca.result=scaled_pca, pca.data=pca_input, individuals.label=F)
directions
# Plot the contribution of the variables to a given PC.
# Variables' labels aligned on the X axis.
loadings_aligned <- LoadingsPlot(pca.result=scaled_pca,  whichPC="PC1",
positive.color="green2", negative.color="grey70", labels.aligned= TRUE)
loadings_aligned
# Variables' labels are placed right below the bars.
LoadingsPlot(pca.result=scaled_pca,  whichPC="PC1",
positive.color="green2", negative.color="grey70", labels.aligned= FALSE)
SaveInputAndPCs(input = "eg_data/NHANES/NHANES_2days_totals_QCed_sampled.txt",
pca.results = scaled_pca,
out.fn = "results/PCA_results/2 days 50 ind//ind50_2days_totalsinput_QCed_PCs.txt")
# Import source code to run the analyses to follow.
source("lib/load_and_check.R")
source("lib/k-means.R")
# ---------------------------------------------------------------------------------------------------------------
# Define your input file. Need to scale it to accommodate measurements in different units.
colnames(selected_variables)
kmeans_input <- scale(selected_variables) # correlated variables removed.
# Set your ggplot2 theme.
require(ggplot2)
theme_set(theme_bw(base_size = 14))
# ---------------------------------------------------------------------------------------------------------------
# Use the elbow method to find the ideal K.
ElbowMethod(k.values = 1:15)
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K.  Uses cluster package.
SilhouetteMethod(k.values = 2:15)
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K.  Uses cluster package.
SilhouetteMethod(k.values = 2:15)
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K.  Uses cluster package.
SilhouetteMethod(k.values = 2:15)
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K.  Uses cluster package.
SilhouetteMethod(k.values = 2:15)
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K.  Uses cluster package.
SilhouetteMethod(k.values = 2:15)
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K.  Uses cluster package.
SilhouetteMethod(k.values = 2:15)
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K.  Uses cluster package.
SilhouetteMethod(k.values = 2:15)
# ---------------------------------------------------------------------------------------------------------------
# Use the Gap statistic method to find the ideal K.
set.seed(123)
GapMethod(k.values = 1:15)
GapMethod(k.values = 1:15)
GapMethod(k.values = 1:15)
FactoextraGapMethod(k.values = 1:15)
FactoextraGapMethod(k.values = 1:15)
# ---------------------------------------------------------------------------------------------------------------
# Perform k-means analysis with one specified k.
One_K(myK = 6)
# ---------------------------------------------------------------------------------------------------------------
# Perform k-means analysis with one specified k.
One_K(myK = 5)
# ---------------------------------------------------------------------------------------------------------------
# Perform k-means analysis with multiple (2-4) Ks, and plot them in one window.
MultipleK(myKs = c(2,3,4,5))
# Create a scree plot.
screep <- LineScreePlot(pca.data = pca_input, pca.result = scaled_pca)
screep
ggsave("results/PCA_results/2 days 50 ind/2days_50ind_screep.png", screep, device="png", width=5, height=5, dpi=200)
# Create a biplot.
# A biplot with the individuals as black dots and variables labelled.
biplotdots <- BiplotDots(pca.result = scaled_pca, pca.data = pca_input)
biplotdots
ggsave("results/PCA_results/2 days 50 ind/2days_50ind_biplotdots.png", biplotdots, device="png", width=5, height=5, dpi=200)
# A biplot with the individuals labeled.
biplotlabeled <- BiplotLabeled(pca.result=scaled_pca, pca.data=pca_input, individuals.label = T)
biplotlabeled
ggsave("results/PCA_results/2 days 50 ind/2days_50ind_biplotlabeled.png", biplotlabeled, device="png", width=5, height=5, dpi=200)
# A biplot with the individuals labeled without the variables' arrows.
biplotlabeledwoarrows <- BiplotLabeledwoArrows(pca.result = scaled_pca,
pca.data = pca_input,
individuals.label = T)
biplotlabeledwoarrows
ggsave("results/PCA_results/2 days 50 ind/2days_50ind_biplotlabeledwoarrows.png", biplotlabeledwoarrows, device="png", width=5, height=5, dpi=200)
# Plot the directions of the variables.
directions <- BiplotLabeled(pca.result=scaled_pca, pca.data=pca_input, individuals.label=F)
directions
ggsave("results/PCA_results/2 days 50 ind/2days_50ind_directions.png", directions, device="png", width=5, height=5, dpi=200)
# Plot the contribution of the variables to a given PC.
# Variables' labels aligned on the X axis.
loadings_aligned <- LoadingsPlot(pca.result=scaled_pca,  whichPC="PC1",
positive.color="green2", negative.color="grey70", labels.aligned= TRUE)
# ---------------------------------------------------------------------------------------------------------------
# Calculate loadings of each PC to the variables and
# save it as a txt file in the results folder.
# Change the file name as necessary.
SaveLoadings(pca.result=scaled_pca,
out.fn = "results/PCA_results/2 days 50 ind/2days_PC_loadings_50ind.txt")
