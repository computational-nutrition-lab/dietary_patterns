head(selected_variables, 1)
dim( selected_variables)
# original
head(subsetted_non0var, 1)
dim( subsetted_non0var)
# ---------------------------------------------------------------------------------------------------------------
# Save the selected_variables as a .txt file. This will be the input for clustering analyses.
write.table(x=selected_variables, file="VVKAJ_Tot_m_QCed_Cat_ave_subset.txt", sep="\t", row.names=F, quote=F)
# ---------------------------------------------------------------------------------------------------------------
# Save the correlation matrix for record in the results folder.
# cc is the correlation matrix produced when variables are collapsed by correlation by using
# the CollapseByCorrelation function.
SaveCorrMatrix(x=cc, out.fn = "VVKAJ_Tot_m_QCed_Cat_ave_corr_matrix.txt")
# ---------------------------------------------------------------------------------------------------------------
# Food category data averaged and processed for clustering analyses.
Tot_m_QCed_Cat_ave <- read.table(file="VVKAJ_Tot_m_QCed_Cat_ave_subset.txt", sep="\t", header=T)
# Name your input data.
pca_input <- Tot_m_QCed_Cat_ave
# Ensure your input file has the correct number of rows and columns.
dim(pca_input)
# Perform PCA with the subset data, scaled.
scaled_pca <- prcomp(x=pca_input, scale = TRUE)
# Specify the directory (folder) to save the results.
res_dir = "Cat_ave_PCA"
# Specify the prefix of filenames to be saved.
res_prefix = "VVKAJ_Cat_ave_2"
# Perform PCA and save the results in a specified folder (out.dir) and a prefix (out.prefix).
# Input is your items/totals input file before any prep for clustering, from which you derived the input for the PCA.
PerformPCA(pca.data=pca_input, pca.result=scaled_pca, out.dir= res_dir, out.prefix= res_prefix)
setwd("~/GitHub/dietary_patterns")
# Name your main directory for future use.
main_wd <- file.path(getwd())
# Import source code to run the analyses to follow.
source("lib/specify_dir_and_check_col.R")
source("lib/prep_data_for_clustering.R")
# ---------------------------------------------------------------------------------------------------------------
# Specify the directory where the data is.
# SpecifyDataDirectory(directory.name = "eg_data/dietstudy/")
SpecifyDataDirectory(directory.name = "eg_data/VVKAJ/")
# ASA24 data ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Load the totals data:
# totals <- read.table("Totals_to_use.txt", sep = "\t", header = T)
totals <- read.table("VVKAJ_Tot_m_QCed.txt", sep = "\t", header = T)
# Define your input dataset (may not be necessary, but keeping this because I'm not
# sure how totals_selected are made. I think it's the same as totals_QC, though.)
totals_selected <- totals
# Subset nutrients data.
# The columns specified as start.col, end.col, and all columns in between will be selected.
# Nutrients analysis --> start.col = "PROT",  end.col = "B12_ADD", 64 variables in total.
SubsetColumns(data = totals_selected, start.col = "PROT", end.col = "B12_ADD")
# Pick up only the columns with non-zero variance, in order to run PCA, cluster analysis etc.
# The removed columns will be shown if any.
KeepNonZeroVarColumns(data = subsetted)
# ---------------------------------------------------------------------------------------------------------------
# Collapse variables by correlation: take only one variables if they are highly correlated.
cbc_res <- CollapseByCorrelation(x = subsetted_non0var, min.cor = 0.75,
select.rep.fcn = 'mean', verbose = T)
# Filter out highly correlated variables from the original dataset.
selected_variables <- subsetted_non0var[, cbc_res$reps]
# Check the name of the original and filtered variables.
# Among the variables in the same group, the one with the highest variance is kept
#  (according to the explanation above.)
# filtered
head(selected_variables, 1)
dim( selected_variables)
# original
head(subsetted_non0var, 1)
dim( subsetted_non0var)
# ---------------------------------------------------------------------------------------------------------------
# Save the selected_variables as a .txt file. This will be the input for clustering analyses.
write.table(x=selected_variables, file="VVKAJ_Tot_m_QCed_Nut_asis.txt", sep="\t", row.names=F, quote=F)
# ---------------------------------------------------------------------------------------------------------------
# Save the correlation matrix for record in the results folder.
# cc is the correlation matrix produced when variables are collapsed by correlation by using
# the CollapseByCorrelation function.
SaveCorrMatrix(x=cc, out.fn = "VVKAJ_Tot_m_QCed_Nut_asis_corr_matrix.txt")
# ===============================================================================================================
# NUTRIENTS: Take average of each user across all days
# ===============================================================================================================
# Specify the data to be used, category to group by, and the range of columns (variables)
# to calculate the means of each variable.
# Nutrients analysis  --> start.col = "PROT",    end.col = "B12_ADD"
AverageBy(data= totals_selected, by= "UserName", start.col= "PROT", end.col= "B12_ADD")
# Save the averaged results.
write.table(x=meansbycategorydf, "VVKAJ_Tot_m_QCed_Nut_ave_allvar.txt", sep="\t", row.names=F, quote=F)
# The column names should be the same as start.col-end.col.
colnames(meansbycategorydf)
# The 'UserName' column has the users to calculate means for.
meansbycategorydf$UserName
# Pick up only the columns with non-zero variance, in order to run PCA and cluster analysis etc.
# The removed columns will be shown if any.
# [,-1] is to exclude the UserName columns that is not numeric and not used for variance calculation.
KeepNonZeroVarColumns(data = meansbycategorydf[, -1])
# ---------------------------------------------------------------------------------------------------------------
# Collapse variables by correlation: take only one variables if they are highly correlated.
cbc_res <- CollapseByCorrelation(x = subsetted_non0var, min.cor = 0.75,
select.rep.fcn = 'mean', verbose = T)
# Filter out highly correlated variables from the original dataset.
selected_variables <- subsetted_non0var[, cbc_res$reps]
# Check the name of the original and filtered variables.
# Among the variables in the same group, the one with the highest variance is kept
#  (according to the explanation above.)
# filtered
head(selected_variables, 1)
dim( selected_variables)
# original
head(subsetted_non0var, 1)
dim( subsetted_non0var)
selected_variables
# ---------------------------------------------------------------------------------------------------------------
# Save the selected_variables as a .txt file. This will be the input for clustering analyses.
write.table(x=selected_variables, file="VVKAJ_Tot_m_QCed_Nut_ave_subset.txt", sep="\t", row.names=F, quote=F)
# ===============================================================================================================
# FOOD CATEGORIES: Use data as is.
# ===============================================================================================================
# Subset food items data.
# The columns specified as start.col, end.col, and all columns in between will be selected.
# Food items analysis --> start.col = "F_TOTAL", end.col = "A_DRINKS", 37 varialbes in total.
SubsetColumns(data = totals_selected, start.col = "F_TOTAL", end.col = "A_DRINKS")
# Pick up only the columns with non-zero variance, in order to run PCA, cluster analysis etc.
# The removed columns will be shown if any.
KeepNonZeroVarColumns(data = subsetted)
# ---------------------------------------------------------------------------------------------------------------
# Collapse variables by correlation: take only one variables if they are highly correlated.
cbc_res <- CollapseByCorrelation(x = subsetted_non0var, min.cor = 0.75,
select.rep.fcn = 'mean', verbose = T)
# Filter out highly correlated variables from the original dataset.
selected_variables <- subsetted_non0var[, cbc_res$reps]
# Check the name of the original and filtered variables.
# Among the variables in the same group, the one with the highest variance is kept
#  (according to the explanation above.)
# filtered
head(selected_variables, 1)
dim( selected_variables)
# original
head(subsetted_non0var, 1)
dim( subsetted_non0var)
# ---------------------------------------------------------------------------------------------------------------
# Save the selected_variables as a .txt file. This will be the input for clustering analyses.
write.table(x=selected_variables, file="VVKAJ_Tot_m_QCed_Cat_asis.txt", sep="\t", row.names=F, quote=F)
# ---------------------------------------------------------------------------------------------------------------
# Save the correlation matrix for record in the results folder.
# cc is the correlation matrix produced when variables are collapsed by correlation by using
# the CollapseByCorrelation function.
SaveCorrMatrix(x=cc, out.fn = "VVKAJ_Tot_m_QCed_Cat_asis_corr_matrix.txt")
# ===============================================================================================================
# FOOD CATEGORIES: Take average of each user across all days
# ===============================================================================================================
# Specify the data to be used, category to group by, and the range of columns (variables)
# to calculate the means of each variable.
# Food items analysis --> start.col = "F_TOTAL", end.col = "A_DRINKS"
AverageBy(data= totals_selected, by= "UserName", start.col= "F_TOTAL", end.col= "A_DRINKS")
# Save the averaged results.
write.table(x=meansbycategorydf, "VVKAJ_Tot_m_QCed_Cat_ave_allvar.txt", sep="\t", row.names=F, quote=F)
# The column names should be UserName + start.col-end.col.
colnames(meansbycategorydf)
# The 'UserName' column has the users to calculate means for.
meansbycategorydf$UserName
# Pick up only the columns with non-zero variance, in order to run PCA and cluster analysis etc.
# The removed columns will be shown if any.
# [,-1] is to exclude the UserName columns that is not numeric and not used for variance calculation.
KeepNonZeroVarColumns(data = meansbycategorydf[, -1])
# ---------------------------------------------------------------------------------------------------------------
# Collapse variables by correlation: take only one variables if they are highly correlated.
cbc_res <- CollapseByCorrelation(x = subsetted_non0var, min.cor = 0.75,
select.rep.fcn = 'mean', verbose = T)
# Filter out highly correlated variables from the original dataset.
selected_variables <- subsetted_non0var[, cbc_res$reps]
# Check the name of the original and filtered variables.
# Among the variables in the same group, the one with the highest variance is kept
#  (according to the explanation above.)
# filtered
head(selected_variables, 1)
dim( selected_variables)
# original
head(subsetted_non0var, 1)
dim( subsetted_non0var)
# ---------------------------------------------------------------------------------------------------------------
# Save the selected_variables as a .txt file. This will be the input for clustering analyses.
write.table(x=selected_variables, file="VVKAJ_Tot_m_QCed_Cat_ave_subset.txt", sep="\t", row.names=F, quote=F)
# ---------------------------------------------------------------------------------------------------------------
# Save the correlation matrix for record in the results folder.
# cc is the correlation matrix produced when variables are collapsed by correlation by using
# the CollapseByCorrelation function.
SaveCorrMatrix(x=cc, out.fn = "VVKAJ_Tot_m_QCed_Cat_ave_corr_matrix.txt")
# ===============================================================================================================
# Come back to the main directory
setwd(main_wd)
# Name your input data.
pca_input <- Tot_m_QCed_Nut_asis
# Nutrient data as is, processed for clustering analyses.
Tot_m_QCed_Nut_asis <- read.table(file="VVKAJ_Tot_m_QCed_Nut_asis.txt", sep="\t", header=T)
# Specify the directory where the data is.
SpecifyDataDirectory(directory.name = "eg_data/VVKAJ/")
ggplot2::theme_set(theme_bw(base_size = 14))
# ---------------------------------------------------------------------------------------------------------------
# Come back to the main directory before you start running another script.
setwd(main_wd)
# Import source code to run the analyses to follow.
# source("lib/specify_dir_and_check_col.R")
# source("lib/prep_data_for_clustering.R")
source("lib/PCA.R")
# Define ggplot themes to use in creating plots.
library(ggplot2)
ggplot2::theme_set(theme_bw(base_size = 14))
# Specify the directory where the data is.
SpecifyDataDirectory(directory.name = "eg_data/VVKAJ/")
# Nutrient data as is, processed for clustering analyses.
Tot_m_QCed_Nut_asis <- read.table(file="VVKAJ_Tot_m_QCed_Nut_asis.txt", sep="\t", header=T)
# Name your input data.
pca_input <- Tot_m_QCed_Nut_asis
# Perform PCA and save the results in a specified folder (out.dir) and a prefix (out.prefix).
PerformPCA(pca.data=pca_input, pca.result=scaled_pca, out.dir= res_dir, out.prefix= res_prefix )
# Perform PCA with the subset data, scaled.
scaled_pca <- prcomp(x=pca_input, scale = TRUE)
# Ensure your input file has the correct number of rows and columns.
dim(pca_input)
# Specify the directory (folder) to save the results.
res_dir = "Nut_asis_PCA"
# Specify the prefix of filenames to be saved.
res_prefix = "VVKAJ_Nut_asis"
# Perform PCA and save the results in a specified folder (out.dir) and a prefix (out.prefix).
PerformPCA(pca.data=pca_input, pca.result=scaled_pca, out.dir= res_dir, out.prefix= res_prefix )
# Combine the input (totals before processing) with all the variables and the PC results.
SaveInputAndPCs(input="VVKAJ_Tot_m_QCed.txt", pca.results = scaled_pca,
out.dir= res_dir, out.prefix= res_prefix)
Tot_m_QCed_Nut_asis <- read.table(file="VVKAJ_Tot_m_QCed_Nut_asis.txt", sep="\t", header=T)
# Ensure your input file has the correct number of rows and columns.
dim(pca_input)
# Perform PCA with the subset data, scaled.
scaled_pca <- prcomp(x=pca_input, scale = TRUE)
# Specify the directory (folder) to save the results.
res_dir = "Nut_asis_PCA"
# Specify the prefix of filenames to be saved.
res_prefix = "VVKAJ_Nut_asis"
# Perform PCA and save the results in a specified folder (out.dir) and a prefix (out.prefix).
PerformPCA(pca.data=pca_input, pca.result=scaled_pca, out.dir= res_dir, out.prefix= res_prefix )
# Combine the input (totals before processing) with all the variables and the PC results.
SaveInputAndPCs(input="VVKAJ_Tot_m_QCed.txt", pca.results = scaled_pca,
out.dir= res_dir, out.prefix= res_prefix)
Tot_m_QCed_Nut_ave <- read.table(file="VVKAJ_Tot_m_QCed_Nut_ave.txt", sep="\t", header=T)
# Name your input data.
pca_input <- Tot_m_QCed_Nut_ave
# Ensure your input file has the correct number of rows and columns.
dim(pca_input)
# Perform PCA with the subset data, scaled.
scaled_pca <- prcomp(x=pca_input, scale = TRUE)
# Specify the directory (folder) to save the results.
res_dir = "Nut_ave_PCA"
# Specify the prefix of filenames to be saved.
res_prefix = "VVKAJ_Nut_ave"
# Perform PCA and save the results in a specified folder (out.dir) and a prefix (out.prefix).
# Input is your items/totals input file before any prep for clustering, from which you derived the input for the PCA.
PerformPCA(pca.data=pca_input, pca.result=scaled_pca, out.dir= res_dir, out.prefix= res_prefix)
# Combine the input (totals before processing) with all the variables and the PC results.
# In the case of aveaged totals data / user, the input file used here is xxx_ave_allvar.txt, which
# has all the variables before filtering out by correlation or zero variance.
SaveInputAndPCs(input="VVKAJ_Tot_m_QCed_Nut_ave_allvar.txt", pca.results = scaled_pca,
out.dir= res_dir, out.prefix= res_prefix)
# ===============================================================================================================
# Food Category data as is, processed for clustering analyses.
# ===============================================================================================================
Tot_m_QCed_Cat_asis <- read.table(file="VVKAJ_Tot_m_QCed_Cat_asis.txt", sep="\t", header=T)
# Name your input data.
pca_input <- Tot_m_QCed_Cat_asis
# Ensure your input file has the correct number of rows and columns.
dim(pca_input)
# Perform PCA with the subset data, scaled.
scaled_pca <- prcomp(x=pca_input, scale = TRUE)
# Specify the directory (folder) to save the results.
res_dir = "Cat_asis_PCA"
# Specify the prefix of filenames to be saved.
res_prefix = "VVKAJ_Cat_asis"
# Perform PCA and save the results in a specified folder (out.dir) and a prefix (out.prefix).
PerformPCA(pca.data=pca_input, pca.result=scaled_pca, out.dir= res_dir, out.prefix= res_prefix )
# Combine the input (totals before processing) with all the variables and the PC results.
SaveInputAndPCs(input="VVKAJ_Tot_m_QCed.txt", pca.results = scaled_pca,
out.dir= res_dir, out.prefix= res_prefix)
# ===============================================================================================================
# Food category data averaged and processed for clustering analyses.
# ===============================================================================================================
Tot_m_QCed_Cat_ave <- read.table(file="VVKAJ_Tot_m_QCed_Cat_ave_subset.txt", sep="\t", header=T)
# Name your input data.
pca_input <- Tot_m_QCed_Cat_ave
# Ensure your input file has the correct number of rows and columns.
dim(pca_input)
# Perform PCA with the subset data, scaled.
scaled_pca <- prcomp(x=pca_input, scale = TRUE)
# Specify the directory (folder) to save the results.
res_dir = "Cat_ave_PCA"
# Specify the prefix of filenames to be saved.
res_prefix = "VVKAJ_Cat_ave_2"
# Specify the prefix of filenames to be saved.
res_prefix = "VVKAJ_Cat_ave"
# Perform PCA and save the results in a specified folder (out.dir) and a prefix (out.prefix).
# Input is your items/totals input file before any prep for clustering, from which you derived the input for the PCA.
PerformPCA(pca.data=pca_input, pca.result=scaled_pca, out.dir= res_dir, out.prefix= res_prefix)
# Combine the input (totals before processing) with all the variables and the PC results.
# In the case of aveaged totals data / user, the input file used here is xxx_ave_allvar.txt, which
# has all the variables before filtering out by correlation or zero variance.
SaveInputAndPCs(input="VVKAJ_Tot_m_QCed_Cat_ave_allvar.txt", pca.results = scaled_pca,
out.dir= res_dir, out.prefix= res_prefix)
Tot_m_QCed_Nut_asis <- read.table(file="VVKAJ_Tot_m_QCed_Nut_asis.txt", sep="\t", header=T)
# Name your input data.
pca_input <- Tot_m_QCed_Nut_asis
# Ensure your input file has the correct number of rows and columns.
dim(pca_input)
# Perform PCA with the subset data, scaled.
scaled_pca <- prcomp(x=pca_input, scale = TRUE)
# Create a scree plot.
screep <- LineScreePlot(pca.data = pca_input, pca.result = scaled_pca)
screep
# ===============================================================================================================
# Come back to the main directory
setwd(main_wd)
# Import source code to run the analyses to follow.
source("lib/load_and_check.R")
# Come back to the main directory
setwd(main_wd)
# Import source code to run the analyses to follow.
source("lib/load_and_check.R")
source("lib/k-means.R")
selected_variables <- read.table("VVKAJ_Tot_m_QCed_Cat_ave_subset.txt", sep="\t", header=F)
# ---------------------------------------------------------------------------------------------------------------
# Specify the directory where the data is.
SpecifyDataDirectory(directory.name = "eg_data/VVKAJ/")
# Define your input file. Need to scale it to accommodate measurements in different units.
selected_variables <- read.table("VVKAJ_Tot_m_QCed_Cat_ave_subset.txt", sep="\t", header=F)
colnames(selected_variables)
# Define your input file. Need to scale it to accommodate measurements in different units.
selected_variables <- read.table("VVKAJ_Tot_m_QCed_Cat_ave_subset.txt", sep="\t", header=T)
colnames(selected_variables)
# Define your input file. Need to scale it to accommodate measurements in different units.
selected_variables <- read.table("VVKAJ_Tot_m_QCed_Nut_asis.txt", sep="\t", header=T)
# Check the column names (variables)
colnames(selected_variables)
# Define it as an input for k-means analysis.
kmeans_input <- scale(selected_variables) # correlated variables removed.
# ---------------------------------------------------------------------------------------------------------------
# Use the elbow method to find the ideal K.
ElbowMethod(k.values = 1:15)
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K.  Uses cluster package.
SilhouetteMethod(k.values = 2:15)
# or use the factoextra package to use the Silhouette method.
factoextra::fviz_nbclust(kmeans_input, kmeans, method="silhouette")
SilhouetteMethod(k.values = 2:15)
# or use the factoextra package to use the Silhouette method.
factoextra::fviz_nbclust(kmeans_input, kmeans, method="silhouette")
# ---------------------------------------------------------------------------------------------------------------
# Use the Gap statistic method to find the ideal K.
set.seed(123)
GapMethod(k.values = 1:15)
# Or use the factoextra package to use the Gap statistic method.
set.seed(123)
FactoextraGapMethod(k.values = 1:15)
# ---------------------------------------------------------------------------------------------------------------
# Perform k-means analysis with one specified k.
One_K(myK = 5)
# ---------------------------------------------------------------------------------------------------------------
# Perform k-means analysis with multiple (2-4) Ks, and plot them in one window.
MultipleK(myKs = c(3,4,5,6))
selected_variables <- read.table("VVKAJ_Tot_m_QCed_Nut_ave_subset.txt", sep="\t", header=T)
# Check the column names (variables)
colnames(selected_variables)
# Define it as an input for k-means analysis.
kmeans_input <- scale(selected_variables) # correlated variables removed.
# Set your ggplot2 theme.
require(ggplot2)
theme_set(theme_bw(base_size = 14))
# ---------------------------------------------------------------------------------------------------------------
# Use the elbow method to find the ideal K.
ElbowMethod(k.values = 1:15)
# ---------------------------------------------------------------------------------------------------------------
# Use the elbow method to find the ideal K.
ElbowMethod(k.values = 1:10)
# ---------------------------------------------------------------------------------------------------------------
# Use the elbow method to find the ideal K.
ElbowMethod(k.values = 1:9)
SilhouetteMethod(k.values = 2:9)
# or use the factoextra package to use the Silhouette method.
factoextra::fviz_nbclust(kmeans_input, kmeans, method="silhouette")
# ---------------------------------------------------------------------------------------------------------------
# Use the Gap statistic method to find the ideal K.
set.seed(123)
GapMethod(k.values = 1:9)
# Or use the factoextra package to use the Gap statistic method.
set.seed(123)
FactoextraGapMethod(k.values = 1:15)
FactoextraGapMethod(k.values = 1:9)
# ---------------------------------------------------------------------------------------------------------------
# Perform k-means analysis with one specified k.
One_K(myK = 5)
# ---------------------------------------------------------------------------------------------------------------
# Perform k-means analysis with multiple (2-4) Ks, and plot them in one window.
MultipleK(myKs = c(3,4,5,6))
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K.  Uses cluster package.
SilhouetteMethod(k.values = 2:8)
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K.  Uses cluster package.
SilhouetteMethod(k.values = 2:9)
# or use the factoextra package to use the Silhouette method.
factoextra::fviz_nbclust(kmeans_input, kmeans, method="silhouette")
source("lib/Food_tree_scripts/make.dhydrt.otu.r")
# use this working directory until this script is complete.
setwd("~/GitHub/dietary_patterns")
# Name your main directory for future use.
main_wd <- file.path(getwd())
# Come back to the main directory
setwd(main_wd)
# ========================================================================================
# Load source scripts
# ========================================================================================
source("lib/Food_tree_scripts/newick.tree.r")
source("lib/Food_tree_scripts/check.db.r")
source("lib/Food_tree_scripts/format.foods.r")
source("lib/Food_tree_scripts/filter.db.by.diet.records.r")
source("lib/Food_tree_scripts/make.food.tree.r")
source("lib/Food_tree_scripts/make.food.otu.r")
source("lib/Food_tree_scripts/make.fiber.otu.r")
source("lib/Food_tree_scripts/make.dhydrt.otu.r")
# Current ASA24 database doesn't have modcodes, so de-duplicate database file,
# replace special characters with "_", and create a new FoodID out of foodcode and modcode.
# It leaves all other columns intact.
FormatFoods(input_fn="data/Food_tree_data/all.food.desc.txt", output_fn="data/Food_tree_data/ASA24Database.txt")
# FoodCode and Main.food.description of additional foods not in ASA24. Format it for use.
FormatFoods(input_fn="data/Food_tree_data/Soylent_codes.txt", output_fn="data/Food_tree_data/Soylent_codes_formatted.txt")
# Specify the directory where the data is.
SpecifyDataDirectory(directory.name = "eg_data/VVKAJ/")
# Format your items data.
FormatFoods(input_fn="VVKAJ_Items_f_s_m.txt", output_fn="Foodtree/VVKAJ_Items_f_s_m_ff.txt", dedupe=F)
# ---------------------------------------------------------------------------------------------------------------
# Generate a tree with the whole ASA24 food database first.
# if there are missing foods, then create new files to add them in below under addl_foods
MakeFoodTree(nodes_fn=        "data/Food_tree_data/NodeLabelsMCT.txt",
food_database_fn="data/Food_tree_data/ASA24Database.txt",
addl_foods_fn= c("data/Food_tree_data/Soylent_codes_formatted.txt"),
num.levels = 4,  # How many levels of foods to be classified
output_taxonomy_fn = "results/Food_tree_results/mct_Lv4.taxonomy.txt",  # Name your output taxonomy file
output_tree_fn=      "results/Food_tree_results/mct_Lv4.tree.nwk"       # Name your output tree
)
# ---------------------------------------------------------------------------------------------------------------
# Come back to the main directory for now.
setwd(main_wd)
# Generate a tree with the whole ASA24 food database first.
# if there are missing foods, then create new files to add them in below under addl_foods
MakeFoodTree(nodes_fn=        "data/Food_tree_data/NodeLabelsMCT.txt",
food_database_fn="data/Food_tree_data/ASA24Database.txt",
addl_foods_fn= c("data/Food_tree_data/Soylent_codes_formatted.txt"),
num.levels = 4,  # How many levels of foods to be classified
output_taxonomy_fn = "results/Food_tree_results/mct_Lv4.taxonomy.txt",  # Name your output taxonomy file
output_tree_fn=      "results/Food_tree_results/mct_Lv4.tree.nwk"       # Name your output tree
)
# Generate a tree with the whole ASA24 food database first.
# if there are missing foods, then create new files to add them in below under addl_foods
MakeFoodTree(nodes_fn=        "data/Food_tree_data/NodeLabelsMCT.txt",
food_database_fn="data/Food_tree_data/ASA24Database.txt",
addl_foods_fn= c("data/Food_tree_data/Soylent_codes_formatted.txt"),
num.levels = 4,  # How many levels of foods to be classified
output_taxonomy_fn = "results/Food_tree_ASA24/mct_Lv4.taxonomy.txt",  # Name your output taxonomy file
output_tree_fn=      "results/Food_tree_ASA24/mct_Lv4.tree.nwk"       # Name your output tree
)
# ---------------------------------------------------------------------------------------------------------------
# Limit to just the foods reported in your study (formatted dietrecords.txt as the input)
FilterDbByDietRecords(food_database_fn = "data/Food_tree_data/ASA24Database.txt",
food_records_fn  = "Foodtree/VVKAJ_Items_f_s_m_ff.txt",   # output of FormatFoods above.
output_fn = "Foodtree/VVKAJ_Items_f_s_m_ff_database.txt")
# ---------------------------------------------------------------------------------------------------------------
# Limit to just the foods reported in your study (formatted dietrecords.txt as the input)
FilterDbByDietRecords(food_database_fn = "data/Food_tree_data/ASA24Database.txt",
food_records_fn  = "Foodtree/VVKAJ_Items_f_s_m_ff.txt",   # output of FormatFoods above.
output_fn = "Foodtree/VVKAJ_Items_f_s_m_ff_database.txt")
# Format your items data, and save the formatted items file to the "Foodtree" folder.
FormatFoods(input_fn="VVKAJ_Items_f_s_m.txt", output_fn="Foodtree/VVKAJ_Items_f_s_m_ff.txt", dedupe=F)
# Specify the directory where the data is.
SpecifyDataDirectory(directory.name = "eg_data/VVKAJ/")
# Limit to just the foods reported in your study (formatted dietrecords.txt as the input)
FilterDbByDietRecords(food_database_fn = "data/Food_tree_data/ASA24Database.txt",
food_records_fn  = "Foodtree/VVKAJ_Items_f_s_m_ff.txt",   # output of FormatFoods above.
output_fn = "Foodtree/VVKAJ_Items_f_s_m_ff_database.txt")
# Limit to just the foods reported in your study (formatted dietrecords.txt as the input)
FilterDbByDietRecords(food_database_fn = "../data/Food_tree_data/ASA24Database.txt",
food_records_fn  = "Foodtree/VVKAJ_Items_f_s_m_ff.txt",   # output of FormatFoods above.
output_fn        = "Foodtree/VVKAJ_Items_f_s_m_ff_database.txt")
# Limit to just the foods reported in your study (formatted dietrecords.txt as the input)
FilterDbByDietRecords(food_database_fn = "../../data/Food_tree_data/ASA24Database.txt",
food_records_fn  = "Foodtree/VVKAJ_Items_f_s_m_ff.txt",   # output of FormatFoods above.
output_fn        = "Foodtree/VVKAJ_Items_f_s_m_ff_database.txt")
# make a food tree with the reduced data.
MakeFoodTree(nodes_fn=         "../../data/Food_tree_data/NodeLabelsMCT.txt",
food_database_fn= "Foodtree/VVKAJ_Items_f_s_m_ff_database.txt",    # output for FilterDbByDietRecords above.
addl_foods_fn   = NULL,
num.levels      = 2,
output_taxonomy_fn = "Foodtree/VVKAJ_Items_f_s_m_ff_reduced_2Lv.tax.txt",
output_tree_fn=      "Foodtree/VVKAJ_Items_f_s_m_ff_reduced_2Lv.tree.nwk"
)
# Make the standard food otu table with data in gram weights of food.
MakeFoodOtu(food_records_fn=  "Foodtree/VVKAJ_Items_f_s_m_ff.txt",
food_record_id =  "UserName",                       # Specify the ID of your participants
food_taxonomy_fn= "Foodtree/VVKAJ_Items_f_s_m_ff_reduced_2Lv.tax.txt",  # Specify your taxonomy file produced by MakeFoodTree.
output_fn =       "Foodtree/VVKAJ_Items_f_s_m_ff_reduced_2Lv.food.otu.txt")  # Name your output otu file.
# Make a food otu table with data in grams of fiber per food
MakeFiberOtu(food_records_fn=  "Foodtree/VVKAJ_Items_f_s_m_ff.txt",
food_record_id=   "UserName",
food_taxonomy_fn= "Foodtree/VVKAJ_Items_f_s_m_ff_reduced_2Lv.tax.txt",
output_fn=        "Foodtree/VVKAJ_Items_f_s_m_ff_reduced_2Lv.fiber.otu.txt")
# Make a food otu table as dehydrated grams per kcal
MakeDhydrtOtu(food_records_fn=  "Foodtree/VVKAJ_Items_f_s_m_ff.txt",
food_record_id =  "UserName",
food_taxonomy_fn= "Foodtree/VVKAJ_Items_f_s_m_ff_reduced_2Lv.tax.txt",
output_fn =       "Foodtree/VVKAJ_Items_f_s_m_ff_reduced_2Lv.dhydrt.otu.txt")
